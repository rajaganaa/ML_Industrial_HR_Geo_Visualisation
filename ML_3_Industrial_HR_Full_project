# -*- coding: utf-8 -*-
"""Industrial_Human_Resource.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1-xU9HoNtaoElxoamFB3jzgWFUgYh1Bqo

Problem Statement:
In India, the industrial classification of the workforce is essential to understand the distribution of
the labor force across various sectors. The classification of main workers and marginal workers,
other than cultivators and agricultural laborers, by sex and by section, division, and class, has
been traditionally used to understand the economic status and employment trends in the
country. However, the current data on this classification is outdated and may not accurately
reflect the current state of the workforce. The aim of this study is to update the information on
the industrial classification of the main and marginal workers, other than cultivators and
agricultural laborers, by sex and by section, division, and class, to provide relevant and accurate
data for policy making and employment planning
"""

# Mergeall the csv data file provided to youand create dataframe

"""
The classical machine learning tasks like DataExploration,DataCleaning,Feature
Engineering,Model Building and Model Testing
"""

import os
import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sns
from sklearn.preprocessing import LabelEncoder
# Function to merge CSV files in a folder
def merge_csv_files(folder_path):
    try:
        # List all CSV files in the folder
        csv_files = [file for file in os.listdir(folder_path) if file.endswith('.csv')]

        # Initialize an empty list to store DataFrame objects
        dfs = []

        # Read each CSV file and append its DataFrame to the list
        for file in csv_files:
            file_path = os.path.join(folder_path, file)
            try:
                # Try reading the CSV file with different encodings
                df = pd.read_csv(file_path, encoding='utf-8')
            except UnicodeDecodeError:
                try:
                    df = pd.read_csv(file_path, encoding='latin-1')
                except UnicodeDecodeError:
                    df = pd.read_csv(file_path, encoding='ISO-8859-1')
            dfs.append(df)

        # Concatenate all DataFrames into a single DataFrame
        merged_df = pd.concat(dfs, ignore_index=True)

        return merged_df
    except Exception as e:
        print("An error occurred:", e)

# Specify the folder path containing CSV files
folder_path = r"/content/drive/Othercomputers/My Laptop (1)/Desktop/raj007_projects/ML_PROJECT_NLP/drive-download-20240913T081543Z-001"
# Merge CSV files in the folder
merged_df = merge_csv_files(folder_path)
pd.set_option('display.max_colwidth', None)
# Now you have the merged DataFrame containing data from all CSV files in the folder

merged_df

merged_df_3=merged_df.copy()

merged_df_3

merged_df_3.info()

merged_df["Class"].info()

merged_df.info()

merged_df.isnull().sum()

new = merged_df.drop(columns="NIC Name", axis=1)
for column in new.columns:
    unique_values = merged_df[column].unique()
    print(f"'{column}':\n {unique_values}\n")

#Remove the backtick
import pandas as pd
import re

# Assuming your DataFrame is named 'merged_df' and the four columns are 'State Code', 'District Code', 'Division', 'Group', and 'Class'
columns_to_modify = ['State Code', 'District Code', 'Division', 'Group', 'Class']

for column in columns_to_modify:
    merged_df[column] = merged_df[column].astype(str).str.replace(r'`', '', regex=True)

merged_df

# Find outlier using Boxplot

import pandas as pd

# Assuming 'merged_df' is your DataFrame

# Identify columns to drop
columns_to_drop = ['India/States', 'NIC Name','State Code','District Code','India/States','Division','Class','Group']

# Create a new DataFrame with the remaining columns
remaining_data = merged_df.drop(columns=columns_to_drop)

# Access the remaining data using 'x'
x = remaining_data

import seaborn as sns
import matplotlib.pyplot as plt

# Assuming 'x' is your DataFrame

# Calculate the correlation matrix
corr_matrix = x.corr()
plt.figure(figsize=(12,6))
# Create a heatmap
sns.heatmap(corr_matrix, annot=True, cmap='coolwarm')
plt.title('Correlation Matrix')
plt.show()

def plot_box_plots(df, cols):
    num_plots = int((len(cols) - 1) / 16) + 1  # Calculate number of plots
    for plot_num in range(num_plots):
        start_col = plot_num * 16
        end_col = min(start_col + 16, len(cols))  # Limit to 16 columns per plot
        subset_cols = cols[start_col:end_col]
        plt.figure(figsize=(20, 22))

        for i, col in enumerate(subset_cols):
            plt.subplot(4, 4, i + 1)
            sns.boxplot(y=df[col])
            plt.title(col)

        plt.tight_layout()
        plt.show()

# Get column names
columns = x.columns.to_list()

# Call the function
plot_box_plots(x, columns)

# Find outlier using IQR

x.info()



# Calculate quartiles and IQR
Q1 = x.quantile(0.25)
Q3 = x.quantile(0.75)
IQR = Q3 - Q1

# Calculate upper and lower bounds for outliers
upper_bound = Q3 + 1.5 * IQR
lower_bound = Q1 - 1.5 * IQR

# Identify outliers
outliers = x[(x < lower_bound) | (x > upper_bound)]

# Count outliers
num_outliers = outliers.count()

print("Number of outliers:")
print(num_outliers)



import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns

# Function to detect outliers using Z-score
def detect_outliers_zscore(data):
    thres = 3
    mean = np.mean(data)
    std = np.std(data)
    outliers = []
    for i in data:
        z_score = (i - mean) / std
        if np.abs(z_score) > thres:
            outliers.append(i)
    return outliers

# Function to cap outliers using IQR method
def cap_outliers_iqr(df, col):
    Q1 = df[col].quantile(0.25)
    Q3 = df[col].quantile(0.75)
    IQR = Q3 - Q1
    upper_threshold = Q3 + 1.5 * IQR
    lower_threshold = Q1 - 1.5 * IQR
    df[col] = df[col].clip(lower_threshold, upper_threshold)

# List of columns to process
columns = ["Main Workers - Total -  Persons",
    "Main Workers - Total - Males",
    "Main Workers - Total - Females",
    "Main Workers - Rural -  Persons",
    "Main Workers - Rural - Males",
    "Main Workers - Rural - Females",
    "Main Workers - Urban -  Persons",
    "Main Workers - Urban - Males",
    "Main Workers - Urban - Females",
    "Marginal Workers - Total -  Persons",
    "Marginal Workers - Total - Males",
    "Marginal Workers - Total - Females",
    "Marginal Workers - Rural -  Persons",
    "Marginal Workers - Rural - Males",
    "Marginal Workers - Rural - Females",
    "Marginal Workers - Urban -  Persons",
    "Marginal Workers - Urban - Males",
    "Marginal Workers - Urban - Females"
]
# Apply outlier detection and capping to each column
for col in columns:
    # Detect outliers using Z-score
    sample_outliers = detect_outliers_zscore(merged_df[col])
    print(f"Outliers from Z-scores method for {col}: {sample_outliers}")

    # Cap outliers using IQR method
    cap_outliers_iqr(merged_df, col)

# Convert float columns to integers (assuming no significant rounding errors)
for col in columns:
    merged_df[col] = merged_df[col].astype(int)

# Create box plot
plt.figure(figsize=(12, 8))
sns.boxplot(data=merged_df[columns])
plt.xticks(rotation=45)
plt.show()



merged_df_2=merged_df.copy()

merged_df_2.info()

## Handling Categorical Features

merged_df_2

import pandas as pd
from sklearn.preprocessing import LabelEncoder

# Assuming 'merged_df_2' is your DataFrame

# Create a composite column
merged_df_2['State_District'] = merged_df_2['India/States'] + ' - ' + merged_df_2['District Code']

# Label encode the composite column
merged_df_2['State_District_Encoded'] = LabelEncoder().fit_transform(merged_df_2['State_District'])

# Drop the original columns
merged_df_2.drop(['India/States', 'District Code'], axis=1, inplace=True)

# Print unique encoded values
print(merged_df_2['State_District_Encoded'].unique())

merged_df_2.drop(columns=['State_District'], inplace=True)

merged_df_2

import re
import pandas as pd

# Assuming 'merged_df' is your DataFrame and 'column_name' contains the data with quotes

merged_df_2['Class'] = merged_df_2['Class'].astype(str).str.replace('"', '', regex=False)
merged_df_2['Class'] = pd.to_numeric(merged_df_2['Class'], errors='coerce')

# Assuming 'merged_df' is your DataFrame and 'column_name' contains the data with quotes

merged_df_2['Division'] = merged_df_2['Division'].astype(str).str.replace('"', '', regex=False)
merged_df_2['Division'] = pd.to_numeric(merged_df_2['Division'], errors='coerce')

merged_df_2['Group'] = merged_df_2['Group'].astype(str).str.replace('"', '', regex=False)
merged_df_2['Group'] = pd.to_numeric(merged_df_2['Group'], errors='coerce')

merged_df_2['State Code'] = merged_df_2['State Code'].astype(str).str.replace('"', '', regex=False)
merged_df_2['State Code'] = pd.to_numeric(merged_df_2['State Code'], errors='coerce')

merged_df_2['State Code']
merged_df_2['Group']
merged_df_2['Division']
merged_df_2['Class']

import pandas as pd
import matplotlib.pyplot as plt
columns_to_plot = ['State Code', 'Group', 'Division', 'Class']
merged_df_2[columns_to_plot].boxplot()
plt.title('Box Plots for Selected Columns')
plt.ylabel('Values')
plt.xticks(rotation=45)
plt.show()
import seaborn as sns  # For additional styling options

sns.boxplot(data=merged_df_2[['State Code', 'Group', 'Division', 'Class']])
plt.title('Box Plots for Selected Columns (Seaborn)')
plt.show()

merged_df_2.info()

import pandas as pd
from sklearn.model_selection import train_test_split
from sklearn.feature_extraction.text import TfidfVectorizer
from sklearn.preprocessing import OneHotEncoder
from sklearn.compose import ColumnTransformer
from sklearn.pipeline import Pipeline
from sklearn.linear_model import LogisticRegression

from sklearn.metrics import accuracy_score, classification_report

# Assuming 'merged_df_2' is your DataFrame

# Define columns to encode

text_column = 'NIC Name'

# Define preprocessing steps for categorical features and text column

text_transformer = TfidfVectorizer(stop_words='english')

# Create a preprocessor to apply transformations to appropriate columns
preprocessor = ColumnTransformer(
    transformers=[
                ('text', text_transformer, text_column)
    ]
)

# Define the model pipeline for logistic regression
model = Pipeline(steps=[
    ('preprocessor', preprocessor),
    ('classifier', LogisticRegression(max_iter=200))
])

# Split data into features and target
X = merged_df_2.drop(columns=['Class'])  # Features
y = merged_df_2['Class']  # Target

# Split data into training and testing sets
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

# Train the model
model.fit(X_train, y_train)

# Make predictions
y_pred = model.predict(X_test)

# Evaluate the model
accuracy = accuracy_score(y_test, y_pred)
print('Accuracy:', accuracy)
print('Classification Report:')
print(classification_report(y_test, y_pred))

import pandas as pd
from sklearn.model_selection import train_test_split
from sklearn.feature_extraction.text import TfidfVectorizer
from sklearn.preprocessing import OneHotEncoder
from sklearn.compose import ColumnTransformer
from sklearn.pipeline import Pipeline
from sklearn.ensemble import RandomForestClassifier

from sklearn.metrics import accuracy_score, classification_report

# Assuming 'merged_df_2' is your DataFrame

# Define columns to encode

text_column = 'NIC Name'

# Define preprocessing steps for categorical features and text column

text_transformer = TfidfVectorizer(stop_words='english')

# Create a preprocessor to apply transformations to appropriate columns
preprocessor = ColumnTransformer(
    transformers=[
                ('text', text_transformer, text_column)
    ]
)

# Define the model pipeline for logistic regression
model = Pipeline(steps=[
    ('preprocessor', preprocessor),
    ('classifier', RandomForestClassifier(n_estimators=100, random_state=42))
])

# Split data into features and target
X = merged_df_2.drop(columns=['Class'])  # Features
y = merged_df_2['Class']  # Target

# Split data into training and testing sets
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

# Train the model
model.fit(X_train, y_train)

# Make predictions
y_pred = model.predict(X_test)

# Evaluate the model
accuracy = accuracy_score(y_test, y_pred)
print('Accuracy:', accuracy)
print('Classification Report:')
print(classification_report(y_test, y_pred))

# Save DataFrame to CSV
merged_df_3.to_csv('HR.csv', index=False)

from google.colab import files

# Download the file
files.download('HR.csv')
#-----------------------------------------------------------------------------------------------------------------------------------------------------------#

# -*- coding: utf-8 -*-
"""Industrial_HR_NLP.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1q4RjgJMvVD8-k-uuty107fRajrupjE1u
"""

#NLP PART

# UseNaturalLanguageProcessingfor analyzingthevariouscoreindustriesandgroupthebusinesscategorieslikeRetail,Poultry,
#  Agriculture,Manufacturingetc

import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns
import nltk
from nltk.corpus import stopwords
import string
import re
import warnings
warnings.filterwarnings('ignore')

df=pd.read_csv('/content/HR_1.csv')

df

df.info()

df["NIC Name"].unique()

pd.set_option('display.max_colwidth', None)

nic_name_column = df["Class"]

nic_name_column.describe()  # Get summary statistics (if applicable)

nic_name_column.value_counts() # Count occurrences of unique values

# Filter the DataFrame for rows where Class equals 9910
filtered_df = df[df["Class"] == 0]

# Display the filtered DataFrame
filtered_df

df["Division"].unique()
# Count the number of unique values in the Division column
total_unique_divisions = df["Division"].nunique()

# Display the result
print("Total unique divisions:", total_unique_divisions)

df["Group"].unique()
# Count the number of unique values in the Division column
total_unique_Group = df["Group"].nunique()

# Display the result
print("Total unique Group:", total_unique_Group)

df["Class"].unique()
# Count the number of unique values in the Class column
total_unique_Class = df["Class"].nunique()

# Display the result
print("Total unique Class:", total_unique_Class)

import plotly.express as px

# Count the frequency of each unique 'NIC Name'
nic_name_counts = df['NIC Name'].value_counts().reset_index()
nic_name_counts.columns = ['NIC Name', 'Count']

# Plot the top 20 NIC Name categories by frequency
fig = px.bar(
    nic_name_counts.head(20),  # Adjust the number as needed (Top 20 for example)
    x='NIC Name',
    y='Count',
    title='Top 20 NIC Name Categories by Frequency',
    labels={'Count': 'Number of Occurrences'},
    template='plotly_dark'
)

fig.update_layout(xaxis_tickangle=-45)
fig.show()


import os
import pandas as pd
import nltk
from nltk.tokenize import word_tokenize
from nltk.corpus import stopwords
from collections import Counter
from wordcloud import WordCloud
import matplotlib.pyplot as plt

# Download necessary NLTK data
nltk.download('punkt')
nltk.download('stopwords')

# Tokenize and clean text data
text = ' '.join(df['NIC Name'])
tokens = word_tokenize(text)
tokens = [word.lower() for word in tokens if word.isalpha()]
stop_words = set(stopwords.words('english'))
tokens = [word for word in tokens if word not in stop_words]

# Count word frequency
word_freq = Counter(tokens)
top_words = word_freq.most_common(10)

# Create a word cloud using the top words
wordcloud = WordCloud(width=800, height=400, background_color='white').generate_from_frequencies(dict(top_words))

# Display the word cloud
plt.figure(figsize=(20, 5))
plt.imshow(wordcloud, interpolation='bilinear')
plt.title('Word Cloud for Top 10 NIC Names')
plt.axis('off')
plt.show()



import plotly.express as px

# Extract the top 10 most common words and their frequencies
top_words = [pair[0] for pair in word_freq.most_common(10)]
word_counts = [pair[1] for pair in word_freq.most_common(10)]

# Create a bar plot using Plotly
fig = px.bar(x=top_words, y=word_counts, labels={'x': 'Industry', 'y': 'Frequency'},
             title='Top 10 Industries by Frequency')
fig.show()

import os
import pandas as pd
import nltk
from nltk.tokenize import word_tokenize
from nltk.corpus import stopwords
from sklearn.feature_extraction.text import TfidfVectorizer
from sklearn.cluster import KMeans
from wordcloud import WordCloud
import matplotlib.pyplot as plt

# # Download necessary NLTK data
# nltk.download('punkt')
# nltk.download('stopwords')

# TF-IDF Vectorization
tfidf_vectorizer = TfidfVectorizer(stop_words='english')
X_tfidf = tfidf_vectorizer.fit_transform(df['NIC Name'])

# KMeans Clustering
num_clusters = 5  # Adjust the number of clusters as needed
kmeans = KMeans(n_clusters=num_clusters, random_state=42)
df['Cluster'] = kmeans.fit_predict(X_tfidf)

# Generate word clouds for each cluster
for cluster in range(num_clusters):
    text_for_cluster = df[df['Cluster'] == cluster]['NIC Name']
    wordcloud = WordCloud(width=800, height=400, background_color='white').generate(' '.join(text_for_cluster))
    plt.figure(figsize=(10, 5))
    plt.imshow(wordcloud, interpolation='bilinear')
    plt.title(f'Word Cloud for Cluster {cluster}')
    plt.axis('off')
    plt.show()

from sklearn.feature_extraction.text import TfidfVectorizer
# Vectorize the 'NIC Name' column using TfidfVectorizer
vectorizer = TfidfVectorizer(stop_words='english')
nic_name_tfidf = vectorizer.fit_transform(df['NIC Name'])

# Count the frequency of each 'NIC Name'
nic_name_counts = df['NIC Name'].value_counts()

# Get the top 10 most frequent 'NIC Name' categories
top_10_nic_names = nic_name_counts.head(10).index.tolist()

import plotly.express as px

# Filter the DataFrame to include only rows where 'NIC Name' is in the top 10 most frequent categories
top_10_df = df[df['NIC Name'].isin(top_10_nic_names)]

# Create a pie chart to visualize the distribution of the top 10 'NIC Name'
fig = px.pie(
    top_10_df,
    names='NIC Name',
    title='Distribution of Top 10 NIC Name Categories',
    hole=0.3  # This adds a hole in the middle to make it a donut chart (optional)
)

# Show the pie chart
fig.show()

import plotly.express as px

# Assuming 'top_10_df' is a DataFrame containing the top 10 'NIC Name'
fig = px.box(
    top_10_df,
    x='NIC Name',
    y='Main Workers - Total -  Persons',
    title='Box plot of Main Workers - Total - Persons by Top 10 NIC Name'
)

# Adjust layout for better readability
fig.update_layout(
    xaxis_title='NIC Name',
    yaxis_title='Main Workers - Total - Persons',
    xaxis_tickangle=-90,  # Rotate x-axis labels for readability
    width=900,  # Adjust width to match figsize in matplotlib
    height=600
)

# Show the plot
fig.show()

import plotly.express as px

# Assuming 'df' is your DataFrame and contains a 'Cluster' column
fig = px.scatter(
    df,
    x='Main Workers - Total -  Persons',
    y='Main Workers - Rural -  Persons',
    color='Cluster',  # This adds the hue effect for different clusters
    title='Main Workers - Total vs Rural - Clustered',
    color_continuous_scale='viridis',  # Similar to seaborn's 'viridis' palette
    labels={
        'Main Workers - Total -  Persons': 'Main Workers - Total - Persons',
        'Main Workers - Rural -  Persons': 'Main Workers - Rural - Persons'
    }
)

# Update layout for better visualization
fig.update_layout(
    xaxis_title='Main Workers - Total - Persons',
    yaxis_title='Main Workers - Rural - Persons',
    width=900,  # Adjust width to match the previous plot size
    height=600
)

# Show the interactive plot
fig.show()

import plotly.express as px

# Assuming 'df' is your DataFrame and contains a 'Cluster' column
fig = px.scatter(
    df,
    x='Main Workers - Total -  Persons',
    y='Main Workers - Urban -  Persons',
    color='Cluster',  # This adds color differentiation for clusters
    title='Main Workers - Total vs Urban - Clustered',
    color_continuous_scale='viridis',  # Similar to seaborn's 'viridis' palette
    labels={
        'Main Workers - Total -  Persons': 'Main Workers - Total - Persons',
        'Main Workers - Urban -  Persons': 'Main Workers - Urban - Persons'
    }
)

# Update layout for better visualization
fig.update_layout(
    xaxis_title='Main Workers - Total - Persons',
    yaxis_title='Main Workers - Urban - Persons',
    width=900,  # Adjust width
    height=600  # Adjust height
)

# Show the interactive plot
fig.show()

import plotly.express as px

# Assuming 'top_10_df' is your DataFrame with the top 10 'NIC Name' values
fig = px.box(
    top_10_df,
    x='NIC Name',
    y='Marginal Workers - Total -  Persons',
    title='Box plot of Marginal Workers - Total - Persons by Top 10 NIC Name'
)

# Update layout for better visualization
fig.update_layout(
    xaxis_title='NIC Name',
    yaxis_title='Marginal Workers - Total - Persons',
    xaxis_tickangle=-90,  # Rotate x-axis labels by 90 degrees
    width=900,  # Adjust width
    height=600  # Adjust height
)

# Show the interactive plot
fig.show()


import plotly.express as px

# Scatter plot using Plotly
fig = px.scatter(
    top_10_df,
    x='Marginal Workers - Total -  Persons',
    y='Marginal Workers - Rural -  Persons',
    color='Cluster',  # Color by cluster
    title='Marginal Workers - Total vs Rural - Clustered',
    color_continuous_scale='Viridis'  # Use a similar color palette like 'viridis'
)

# Update axis labels
fig.update_layout(
    xaxis_title='Marginal Workers - Total - Persons',
    yaxis_title='Marginal Workers - Rural - Persons',
    width=900,  # Adjust width
    height=600  # Adjust height
)

# Show the interactive plot
fig.show()

import plotly.express as px

# Scatter plot using Plotly
fig = px.scatter(
    top_10_df,
    x='Marginal Workers - Total -  Persons',
    y='Marginal Workers - Urban -  Persons',
    color='Cluster',  # Color points by cluster
    title='Marginal Workers - Total vs Urban - Clustered',
    color_continuous_scale='Viridis'  # Use 'viridis' color palette similar to Seaborn
)

# Update axis labels and figure size
fig.update_layout(
    xaxis_title='Marginal Workers - Total - Persons',
    yaxis_title='Marginal Workers - Urban - Persons',
    width=900,  # Set width
    height=600  # Set height
)

# Show the interactive plot
fig.show()


#--------------------------------------------------------------------------------------------------------------------------------------------------------#

# -*- coding: utf-8 -*-
"""Industrial_HR_Streamlit_part.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1kE0z4eYT0kLKKCUupuVrICteMzUKFlMP
"""

#STREAMLIT VISUALIZATION PART

import os
import pandas as pd
import seaborn as sns
import matplotlib.pyplot as plt
import plotly.express as px
import plotly.graph_objects as go
import requests
import streamlit as st
from sklearn.feature_extraction.text import TfidfVectorizer
from sklearn.cluster import KMeans
from nltk.corpus import stopwords
from nltk.tokenize import word_tokenize
from wordcloud import WordCloud
from collections import Counter
from streamlit_option_menu import option_menu
from PIL import Image
import nltk


# Download NLTK stopwords and punkt tokenizer
nltk.download('stopwords')
nltk.download('punkt')
# Setting up page configuration
# Setting up page configuration
icon = Image.open(r"smart2.jpg")


st.set_page_config(page_title="Industrial Human Resource  | By RAJAGANAPATHY",
                   page_icon=icon,
                   layout="wide",
                   initial_sidebar_state="expanded",
                   menu_items={'About': """# This dashboard app is created by *RAJAGANAPTHY*!"""})

# Creating option menu in the sidebar
with st.sidebar:
    selected = option_menu("Menu", ["Home", "Overview", "Explore"],
                           icons=["house", "graph-up-arrow", "bar-chart-line"],
                           menu_icon="menu-button-wide",
                           default_index=0,
                           styles={"nav-link": {"font-size": "20px", "text-align": "left", "margin": "-2px",
                                                "--hover-color": "#FF5A5F"},
                                   "nav-link-selected": {"background-color": "#FF5A5F"}}
                           )


# Based on the selected option, display the appropriate page
# Function to merge CSV files in a folder
def merge_csv_files(folder_path):
    try:
        # List all CSV files in the folder
        csv_files = [file for file in os.listdir(folder_path) if file.endswith('.csv')]

        # Initialize an empty list to store DataFrame objects
        dfs = []

        # Read each CSV file and append its DataFrame to the list
        for file in csv_files:
            file_path = os.path.join(folder_path, file)
            try:
                # Try reading the CSV file with different encodings
                df = pd.read_csv(file_path, encoding='utf-8-sig')  # Try utf-8-sig encoding
            except UnicodeDecodeError:
                try:
                    df = pd.read_csv(file_path, encoding='latin-1')
                except UnicodeDecodeError:
                    df = pd.read_csv(file_path, encoding='ISO-8859-1')
            dfs.append(df)

        # Concatenate all DataFrames into a single DataFrame
        merged_df = pd.concat(dfs, ignore_index=True)

        return merged_df
    except Exception as e:
        print("An error occurred:", e)


# Specify the folder path containing CSV files
folder_path = r"C:\Users\Dell\OneDrive\Desktop\raj007_projects\ML_PROJECT_NLP\drive-download-20240913T081543Z-001"

# Merge CSV files in the folder
merged_df = merge_csv_files(folder_path)

# Print merged DataFrame for debugging
print("Merged DataFrame:", merged_df)

# Print column names for debugging
print("Column names:", merged_df.columns)

# Print number of rows for debugging
print("Number of rows:", len(merged_df))

# -----------------------------------------------------------------------------------------------------------------------------------------------------#
# Separate state and district names
merged_df[['STATE', 'District']] = merged_df['India/States'].str.split(' - ', expand=True)


# Function to separate state and district names
def separate_state_district(row):
    # Split the string based on the separator '-'
    parts = row.split(' - ')

    # If the first part is in uppercase (assumed to be state name), return it
    if parts[0].isupper():
        return parts[0]
    else:
        return None


# Apply the function to create a new column for state names
merged_df['State Name'] = merged_df['District'].apply(separate_state_district)

# Filter out None values and then print unique state names with commas
state_names = merged_df['State Name'].dropna().unique()
print(", ".join(state_names))

# Create a mapping dictionary for state names
state_name_mapping = {
    'ANDHRA PRADESH': 'Andhra Pradesh',
    'ARUNACHAL PRADESH': 'Arunachal Pradesh',
    'ASSAM': 'Assam',
    'BIHAR': 'Bihar',
    'CHHATTISGARH': 'Chhattisgarh',
    'GOA': 'Goa',
    'GUJARAT': 'Gujarat',
    'HARYANA': 'Haryana',
    'HIMACHAL PRADESH': 'Himachal Pradesh',
    'JAMMU AND KASHMIR': 'Jammu & Kashmir',
    'JHARKHAND': 'Jharkhand',
    'KARNATAKA': 'Karnataka',
    'KERALA': 'Kerala',
    'MADHYA PRADESH': 'Madhya Pradesh',
    'MAHARASHTRA': 'Maharashtra',
    'MANIPUR': 'Manipur',
    'MEGHALAYA': 'Meghalaya',
    'MIZORAM': 'Mizoram',
    'NAGALAND': 'Nagaland',
    'ODISHA': 'Orissa',
    'PUNJAB': 'Punjab',
    'RAJASTHAN': 'Rajasthan',
    'SIKKIM': 'Sikkim',
    'TAMIL NADU': 'Tamil Nadu',
    'TELANGANA': 'Telangana',
    'TRIPURA': 'Tripura',
    'UTTAR PRADESH': 'Uttar Pradesh',
    'UTTARAKHAND': 'Uttaranchal',
    'WEST BENGAL': 'West Bengal',
    'ANDAMAN AND NICOBAR ISLANDS': 'Andaman & Nicobar Island',
    'CHANDIGARH': 'Chandigarh',
    'DADRA AND NAGAR HAVELI AND DAMAN AND DIU': 'Dadra & Nagar Haveli & Daman & Diu',
    'LAKSHADWEEP': 'Lakshadweep',
    'NCT OF DELHI': 'Delhi',
    'PUDUCHERRY': 'Puducherry'
}

# Apply the mapping to normalize state names
merged_df['State Name'] = merged_df['State Name'].apply(lambda x: state_name_mapping.get(x, x))

# Check and print normalized state names
print(merged_df['State Name'].unique())

# ---------------------------------------------------------------------------------------------------------------------------------------------------#

if selected == "Home":
    # Set the title and image for the home page
    st.title("Industrial Human Resource Geo-Visualization")
    image = Image.open(r"img_102705_indian_economy.jpg")
    st.image(image, use_column_width=True)

    # Dataset
    st.subheader("About the Dataset:")
    st.write(
        "Our dataset comprises state-wise counts of main and marginal workers across diverse industries, including manufacturing, construction, retail, and more.")

    # Introduction
    st.write(
        "Explore the dynamic landscape of India's workforce with our Industrial Human Resource Geo-Visualization project.")
    st.write(
        "Gain insights into employment trends, industry distributions, and economic patterns to drive informed decision-making and policy formulation.")

    # Key Features
    st.subheader("Key Features:")
    st.markdown("""
    - **Data Exploration:** Dive deep into state-wise industrial classification data.
    - **Visualization:** Interactive charts and maps for intuitive data exploration.
    - **Natural Language Processing:** Analyze and categorize core industries using NLP techniques.
    - **Insights and Analysis:** Extract actionable insights to support policy-making and resource management.
    """)

    # About the Project
    st.subheader("About the Project:")
    st.write("Our project aims to:")
    st.markdown("""
    - Update and refine the industrial classification data of main and marginal workers.
    - Provide accurate and relevant information for policy-making and employment planning.
    - Empower stakeholders with actionable insights to foster economic growth and development.
    """)

# -------------------------------------------------------------------------------------------------------------------------------------------------------#

if selected == "Overview":

    # Dataset
    st.subheader("Dataset Overview:")
    st.write("Our dataset includes:")
    st.markdown("""
    - State-wise counts of main and marginal workers across various industries.
    - Gender-based distribution of workforce in different sectors.
    - Historical data for trend analysis and forecasting.
    """)

    # Technologies Used
    st.subheader("Technologies Utilized:")
    st.write("We leverage cutting-edge technologies such as:")
    st.markdown("""
    - Python for data processing and analysis.
    - Streamlit for interactive visualization.
    - Plotly and Matplotlib for creating insightful charts.
    - NLTK for Natural Language Processing tasks.
    """)

    # TF-IDF Vectorization
    tfidf_vectorizer = TfidfVectorizer(stop_words='english')
    X_tfidf = tfidf_vectorizer.fit_transform(merged_df['NIC Name'])

    # KMeans Clustering
    num_clusters = 5  # Adjust the number of clusters as needed
    kmeans = KMeans(n_clusters=num_clusters, random_state=42)
    merged_df['Cluster'] = kmeans.fit_predict(X_tfidf)

    # Selectbox for choosing the cluster
    selected_cluster = st.selectbox('Select Cluster', range(num_clusters))

    # Filter text data for the selected cluster
    text_for_cluster = merged_df[merged_df['Cluster'] == selected_cluster]['NIC Name']

    # Tokenize and clean text data
    tokens = word_tokenize(' '.join(text_for_cluster))
    tokens = [word.lower() for word in tokens if word.isalpha()]
    stop_words = set(stopwords.words('english'))
    tokens = [word for word in tokens if word not in stop_words]

    # Count word frequency
    word_freq = Counter(tokens)

    # Generate word cloud for the selected cluster
    wordcloud = WordCloud(width=800, height=400, background_color='white').generate_from_frequencies(dict(word_freq))

    # Display the word cloud in Streamlit
    st.subheader(f'Word Cloud for Cluster {selected_cluster}')
    st.image(wordcloud.to_array(), caption='Word Cloud', use_column_width=True)

    # # Streamlit app
    # st.title('Cluster Distribution')
    #
    # # Visualize the clustering results
    # st.subheader('Distribution of Clusters (Pie Chart)')
    #
    # # Count the occurrences of each cluster
    # cluster_counts = merged_df['Cluster'].value_counts()
    #
    # # Convert counts to a pie chart
    # fig, ax = plt.subplots()
    # ax.pie(cluster_counts, labels=cluster_counts.index, autopct='%1.1f%%', startangle=90)
    # ax.axis('equal')  # Equal aspect ratio ensures that pie is drawn as a circle.
    # st.pyplot(fig)

    # Streamlit app
    st.title('Cluster Distribution')

    # Visualize the clustering results
    st.subheader('Distribution of Clusters (Pie Chart)')

    # Count the occurrences of each cluster
    cluster_counts = merged_df['Cluster'].value_counts()

    # Create a pie chart using Plotly
    fig = px.pie(values=cluster_counts.values, names=cluster_counts.index,
                 title='Distribution of Clusters',
                 labels={'values': 'Count', 'names': 'Clusters'})

    # Display the Plotly chart
    st.plotly_chart(fig)

    # Filter options for work type
    work_type_options = ['Main Workers - Total -  Persons', 'Marginal Workers - Total -  Persons']
    selected_work_type = st.selectbox("Select Work Type:", work_type_options)

    # Filter for top 10 NIC Names based on the selected work type
    top_10_nic_names = merged_df.groupby('NIC Name')[selected_work_type].sum().nlargest(10).index
    top_10_merged_df = merged_df[merged_df['NIC Name'].isin(top_10_nic_names)]

    # Plotting the box plot using Seaborn and Matplotlib
    st.subheader(f'Box Plot of {selected_work_type} by Top 10 NIC Name')

    # Calculate total values for each NIC Name
    top_10_nic_names_totals = top_10_merged_df.groupby('NIC Name')[selected_work_type].sum().reset_index()

    # Create the treemap
    fig = px.treemap(top_10_nic_names_totals, path=['NIC Name'], values=selected_work_type,
                     title=f'Treemap of {selected_work_type} by Top 10 NIC Name')
    st.plotly_chart(fig)

elif selected == "Explore":
    # Tokenize and clean text data
    text = ' '.join(merged_df['NIC Name'])
    tokens = word_tokenize(text)
    tokens = [word.lower() for word in tokens if word.isalpha()]
    stop_words = set(stopwords.words('english'))
    tokens = [word for word in tokens if word not in stop_words]

    # Count word frequency
    word_freq = Counter(tokens)
    top_words = word_freq.most_common(10)

    # TF-IDF Vectorization
    tfidf_vectorizer = TfidfVectorizer(stop_words='english')
    X_tfidf = tfidf_vectorizer.fit_transform(merged_df['NIC Name'])

    # KMeans Clustering
    num_clusters = 5  # Adjust the number of clusters as needed
    kmeans = KMeans(n_clusters=num_clusters, random_state=42)
    merged_df['Cluster'] = kmeans.fit_predict(X_tfidf)

# Streamlit App
    st.title("Industrial Human Resource  Dashboard")

    # Select box for type of worker
    worker_type = st.selectbox('Select Worker Type', ['Main Workers', 'Marginal Workers'])

    # Column mapping
    if worker_type == 'Main Workers':
        column_total = 'Main Workers - Total -  Persons'
        column_rural = 'Main Workers - Rural -  Persons'
        column_urban = 'Main Workers - Urban -  Persons'
    else:
        column_total = 'Marginal Workers - Total -  Persons'
        column_rural = 'Marginal Workers - Rural -  Persons'
        column_urban = 'Marginal Workers - Urban -  Persons'

    # Strip any extra spaces from column names
    merged_df.columns = [col.strip() for col in merged_df.columns]

    # Print DataFrame columns for debugging
    print("DataFrame Columns:", merged_df.columns)

    # Scatter Plot
    fig1 = px.scatter(merged_df, x=column_total, y=column_rural, color='Cluster',
                      title=f'{worker_type} - Total vs Rural')
    st.plotly_chart(fig1)

    fig2 = px.scatter(merged_df, x=column_total, y=column_urban, color='Cluster',
                      title=f'{worker_type} - Total vs Urban')
    st.plotly_chart(fig2)

    # # Box Plot for Top 10 NIC Names
    # top_10_nic_names = merged_df['NIC Name'].value_counts().head(10).index
    # top_10_df = merged_df[merged_df['NIC Name'].isin(top_10_nic_names)]
    #
    # fig3 = px.box(top_10_df, x='NIC Name', y=column_total, title=f'{worker_type} by Top 10 NIC Names')
    # st.plotly_chart(fig3)

    # Pie Chart for Top 10 NIC Names
    top_10_nic_names = merged_df['NIC Name'].value_counts().head(10)
    fig3 = px.pie(top_10_nic_names, values=top_10_nic_names.values, names=top_10_nic_names.index,
                  title=f'{worker_type} Distribution by Top 10 NIC Names')

    st.plotly_chart(fig3)

    # Cluster Distribution
    fig4 = px.histogram(merged_df, x='Cluster', title='Cluster Distribution')
    st.plotly_chart(fig4)

    # # Count plot for a categorical column
    # st.subheader(f"Distribution of {worker_type} by State")
    #
    # # Set the color palette
    # sns.set_palette("bright")  # You can choose different palettes like "pastel", "deep", "bright", etc.
    #
    # # Create the plot
    # fig, ax = plt.subplots()
    # sns.countplot(x='State Name', data=merged_df, ax=ax)
    # plt.xticks(rotation=90)
    # st.pyplot(fig)

    # Bar Chart for Distribution of Worker Type by State
    st.subheader(f"Distribution of {worker_type} by State")

    # Count the occurrences of each state
    state_counts = merged_df['State Name'].value_counts()

    # Create a bar chart using Plotly
    fig = px.bar(state_counts, x=state_counts.index, y=state_counts.values,
                 labels={'x': 'State Name', 'y': 'Count'},
                 title=f'Distribution of {worker_type} by State')

    # Rotate x-axis labels for better visibility
    fig.update_layout(xaxis_tickangle=-90)

    # Display the Plotly chart
    st.plotly_chart(fig)

    # Plot
    st.subheader(f'Relationship between {worker_type} - Rural/Urban - Persons and {worker_type} - Total - Persons')

    # Create the plot
    fig, ax = plt.subplots(figsize=(10, 6))
    ax.scatter(merged_df[f'{worker_type} - Rural -  Persons'], merged_df[f'{worker_type} - Total -  Persons'],
               label='Rural', alpha=0.5)
    ax.scatter(merged_df[f'{worker_type} - Urban -  Persons'], merged_df[f'{worker_type} - Total -  Persons'],
               label='Urban', alpha=0.5)
    ax.set_xlabel(f'{worker_type} - Rural - Persons / {worker_type} - Urban - Persons')
    ax.set_ylabel(f'{worker_type} - Total - Persons')
    ax.set_title(f'Relationship between {worker_type} - Rural/Urban - Persons and {worker_type} - Total - Persons')
    ax.legend()
    ax.grid(True)

    # Display the plot
    st.pyplot(fig)

    # Create a word cloud using the top words
    wordcloud = WordCloud(width=800, height=400, background_color='white').generate_from_frequencies(dict(top_words))

    # Display the word cloud in Streamlit
    st.subheader('Word Cloud for Top 10 NIC Names')
    st.image(wordcloud.to_array(), caption='Word Cloud', use_column_width=True)

    merged_df = merged_df.dropna(subset=['State Name'])


    # Fetch GeoJSON data for India's states
    @st.cache_resource
    def fetch_geojson():
        geojson_url = "https://raw.githubusercontent.com/geohacker/india/master/state/india_state.geojson"
        response = requests.get(geojson_url)
        if response.status_code == 200:
            return response.json()
        else:
            st.error("Failed to fetch GeoJSON data")


    # Main Streamlit App
    def main():
        st.title("India Map Visualization")

        # Fetch GeoJSON data
        geojson_data = fetch_geojson()

        # Extract state names from GeoJSON data
        geojson_state_names = set(feature['properties']['NAME_1'] for feature in geojson_data['features'])

        # State names from DataFrame
        dataframe_state_names = set(merged_df['State Name'])

        # Select box for type of worker
        worker_type = st.selectbox('Select Worker Type', ['Main Workers', 'Marginal Workers'],
                                   key="worker_type_selectbox")

        # Select box for sex
        sex_type = st.selectbox('Select Sex', ['Males', 'Females'], key="sex_type_selectbox")

        # Select box for area
        area_type = st.selectbox('Select Area', ['Rural', 'Urban'], key="area_type_selectbox")

        # Determine the column based on selected worker type, sex, and area
        column_name = f'{worker_type} - {area_type} - {sex_type}'

        # Plotly Choropleth map
        fig = go.Figure(go.Choroplethmapbox(
            geojson=geojson_data,
            locations=merged_df['State Name'],  # Use the column with state names
            featureidkey="properties.NAME_1",  # Key in geojson to match with DataFrame
            z=merged_df[column_name],  # Use the column for analysis
            colorscale='Viridis',
            zmin=merged_df[column_name].min(),
            zmax=merged_df[column_name].max(),
            marker_opacity=0.7,
            marker_line_width=0,
        ))

        fig.update_layout(
            mapbox_style="carto-positron",
            mapbox_zoom=3,
            mapbox_center={"lat": 20.5937, "lon": 78.9629},
            margin={"r": 0, "t": 0, "l": 0, "b": 0},
            title=f"{worker_type} ({sex_type}, {area_type}) Population Across Indian States",
            title_x=0.5
        )

        # Display the map
        st.plotly_chart(fig)

        # Top NIC Names State-wise
        st.title("Top NIC Names State-wise")
        for state in merged_df['State Name'].unique():
            top_nic_name = merged_df[merged_df['State Name'] == state]['NIC Name'].mode()[0]
            st.write(f"Top NIC Name in {state}: {top_nic_name}")


    # Call the main function
    if __name__ == "__main__":
        main()







