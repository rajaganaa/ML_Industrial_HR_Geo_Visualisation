{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyMhiAUMFA/rNYl79/mt0Jzr",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/rajaganaa/ML_Industrial_HR_Geo_Visualisation/blob/main/part_3_Industrial_HR_Streamlit_part.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "BWAIrr5EpTvp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "VqVC4zDqmHd1"
      },
      "outputs": [],
      "source": [
        "#STREAMLIT VISUALIZATION PART"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import pandas as pd\n",
        "import seaborn as sns\n",
        "import matplotlib.pyplot as plt\n",
        "import plotly.express as px\n",
        "import plotly.graph_objects as go\n",
        "import requests\n",
        "import streamlit as st\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from sklearn.cluster import KMeans\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.tokenize import word_tokenize\n",
        "from wordcloud import WordCloud\n",
        "from collections import Counter\n",
        "from streamlit_option_menu import option_menu\n",
        "from PIL import Image\n",
        "import nltk\n",
        "\n",
        "\n",
        "\n",
        "# Download NLTK stopwords and punkt tokenizer\n",
        "nltk.download('stopwords')\n",
        "nltk.download('punkt')\n",
        "# Setting up page configuration\n",
        "# Setting up page configuration\n",
        "icon = Image.open(r\"smart2.jpg\")\n",
        "\n",
        "\n",
        "st.set_page_config(page_title=\"Industrial Human Resource  | By RAJAGANAPATHY\",\n",
        "                   page_icon=icon,\n",
        "                   layout=\"wide\",\n",
        "                   initial_sidebar_state=\"expanded\",\n",
        "                   menu_items={'About': \"\"\"# This dashboard app is created by *RAJAGANAPTHY*!\"\"\"})\n",
        "\n",
        "# Creating option menu in the sidebar\n",
        "with st.sidebar:\n",
        "    selected = option_menu(\"Menu\", [\"Home\", \"Overview\", \"Explore\"],\n",
        "                           icons=[\"house\", \"graph-up-arrow\", \"bar-chart-line\"],\n",
        "                           menu_icon=\"menu-button-wide\",\n",
        "                           default_index=0,\n",
        "                           styles={\"nav-link\": {\"font-size\": \"20px\", \"text-align\": \"left\", \"margin\": \"-2px\",\n",
        "                                                \"--hover-color\": \"#FF5A5F\"},\n",
        "                                   \"nav-link-selected\": {\"background-color\": \"#FF5A5F\"}}\n",
        "                           )\n",
        "\n",
        "\n",
        "# Based on the selected option, display the appropriate page\n",
        "# Function to merge CSV files in a folder\n",
        "def merge_csv_files(folder_path):\n",
        "    try:\n",
        "        # List all CSV files in the folder\n",
        "        csv_files = [file for file in os.listdir(folder_path) if file.endswith('.csv')]\n",
        "\n",
        "        # Initialize an empty list to store DataFrame objects\n",
        "        dfs = []\n",
        "\n",
        "        # Read each CSV file and append its DataFrame to the list\n",
        "        for file in csv_files:\n",
        "            file_path = os.path.join(folder_path, file)\n",
        "            try:\n",
        "                # Try reading the CSV file with different encodings\n",
        "                df = pd.read_csv(file_path, encoding='utf-8-sig')  # Try utf-8-sig encoding\n",
        "            except UnicodeDecodeError:\n",
        "                try:\n",
        "                    df = pd.read_csv(file_path, encoding='latin-1')\n",
        "                except UnicodeDecodeError:\n",
        "                    df = pd.read_csv(file_path, encoding='ISO-8859-1')\n",
        "            dfs.append(df)\n",
        "\n",
        "        # Concatenate all DataFrames into a single DataFrame\n",
        "        merged_df = pd.concat(dfs, ignore_index=True)\n",
        "\n",
        "        return merged_df\n",
        "    except Exception as e:\n",
        "        print(\"An error occurred:\", e)\n",
        "\n",
        "\n",
        "# Specify the folder path containing CSV files\n",
        "folder_path = r\"C:\\Users\\Dell\\OneDrive\\Desktop\\raj007_projects\\ML_PROJECT_NLP\\drive-download-20240913T081543Z-001\"\n",
        "\n",
        "# Merge CSV files in the folder\n",
        "merged_df = merge_csv_files(folder_path)\n",
        "\n",
        "# Print merged DataFrame for debugging\n",
        "print(\"Merged DataFrame:\", merged_df)\n",
        "\n",
        "# Print column names for debugging\n",
        "print(\"Column names:\", merged_df.columns)\n",
        "\n",
        "# Print number of rows for debugging\n",
        "print(\"Number of rows:\", len(merged_df))\n",
        "\n"
      ],
      "metadata": {
        "id": "0IqpMPr2mXji"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# -----------------------------------------------------------------------------------------------------------------------------------------------------#\n",
        "# Separate state and district names\n",
        "merged_df[['STATE', 'District']] = merged_df['India/States'].str.split(' - ', expand=True)\n",
        "\n",
        "\n",
        "# Function to separate state and district names\n",
        "def separate_state_district(row):\n",
        "    # Split the string based on the separator '-'\n",
        "    parts = row.split(' - ')\n",
        "\n",
        "    # If the first part is in uppercase (assumed to be state name), return it\n",
        "    if parts[0].isupper():\n",
        "        return parts[0]\n",
        "    else:\n",
        "        return None\n",
        "\n",
        "\n",
        "# Apply the function to create a new column for state names\n",
        "merged_df['State Name'] = merged_df['District'].apply(separate_state_district)\n",
        "\n",
        "# Filter out None values and then print unique state names with commas\n",
        "state_names = merged_df['State Name'].dropna().unique()\n",
        "print(\", \".join(state_names))\n",
        "\n",
        "# Create a mapping dictionary for state names\n",
        "state_name_mapping = {\n",
        "    'ANDHRA PRADESH': 'Andhra Pradesh',\n",
        "    'ARUNACHAL PRADESH': 'Arunachal Pradesh',\n",
        "    'ASSAM': 'Assam',\n",
        "    'BIHAR': 'Bihar',\n",
        "    'CHHATTISGARH': 'Chhattisgarh',\n",
        "    'GOA': 'Goa',\n",
        "    'GUJARAT': 'Gujarat',\n",
        "    'HARYANA': 'Haryana',\n",
        "    'HIMACHAL PRADESH': 'Himachal Pradesh',\n",
        "    'JAMMU AND KASHMIR': 'Jammu & Kashmir',\n",
        "    'JHARKHAND': 'Jharkhand',\n",
        "    'KARNATAKA': 'Karnataka',\n",
        "    'KERALA': 'Kerala',\n",
        "    'MADHYA PRADESH': 'Madhya Pradesh',\n",
        "    'MAHARASHTRA': 'Maharashtra',\n",
        "    'MANIPUR': 'Manipur',\n",
        "    'MEGHALAYA': 'Meghalaya',\n",
        "    'MIZORAM': 'Mizoram',\n",
        "    'NAGALAND': 'Nagaland',\n",
        "    'ODISHA': 'Orissa',\n",
        "    'PUNJAB': 'Punjab',\n",
        "    'RAJASTHAN': 'Rajasthan',\n",
        "    'SIKKIM': 'Sikkim',\n",
        "    'TAMIL NADU': 'Tamil Nadu',\n",
        "    'TELANGANA': 'Telangana',\n",
        "    'TRIPURA': 'Tripura',\n",
        "    'UTTAR PRADESH': 'Uttar Pradesh',\n",
        "    'UTTARAKHAND': 'Uttaranchal',\n",
        "    'WEST BENGAL': 'West Bengal',\n",
        "    'ANDAMAN AND NICOBAR ISLANDS': 'Andaman & Nicobar Island',\n",
        "    'CHANDIGARH': 'Chandigarh',\n",
        "    'DADRA AND NAGAR HAVELI AND DAMAN AND DIU': 'Dadra & Nagar Haveli & Daman & Diu',\n",
        "    'LAKSHADWEEP': 'Lakshadweep',\n",
        "    'NCT OF DELHI': 'Delhi',\n",
        "    'PUDUCHERRY': 'Puducherry'\n",
        "}\n",
        "\n",
        "# Apply the mapping to normalize state names\n",
        "merged_df['State Name'] = merged_df['State Name'].apply(lambda x: state_name_mapping.get(x, x))\n",
        "\n",
        "# Check and print normalized state names\n",
        "print(merged_df['State Name'].unique())\n",
        "\n",
        "# ---------------------------------------------------------------------------------------------------------------------------------------------------#\n"
      ],
      "metadata": {
        "id": "fkFARBe7mXmA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "if selected == \"Home\":\n",
        "    # Set the title and image for the home page\n",
        "    st.title(\"Industrial Human Resource Geo-Visualization\")\n",
        "    image = Image.open(r\"img_102705_indian_economy.jpg\")\n",
        "    st.image(image, use_column_width=True)\n",
        "\n",
        "    # Dataset\n",
        "    st.subheader(\"About the Dataset:\")\n",
        "    st.write(\n",
        "        \"Our dataset comprises state-wise counts of main and marginal workers across diverse industries, including manufacturing, construction, retail, and more.\")\n",
        "\n",
        "    # Introduction\n",
        "    st.write(\n",
        "        \"Explore the dynamic landscape of India's workforce with our Industrial Human Resource Geo-Visualization project.\")\n",
        "    st.write(\n",
        "        \"Gain insights into employment trends, industry distributions, and economic patterns to drive informed decision-making and policy formulation.\")\n",
        "\n",
        "    # Key Features\n",
        "    st.subheader(\"Key Features:\")\n",
        "    st.markdown(\"\"\"\n",
        "    - **Data Exploration:** Dive deep into state-wise industrial classification data.\n",
        "    - **Visualization:** Interactive charts and maps for intuitive data exploration.\n",
        "    - **Natural Language Processing:** Analyze and categorize core industries using NLP techniques.\n",
        "    - **Insights and Analysis:** Extract actionable insights to support policy-making and resource management.\n",
        "    \"\"\")\n",
        "\n",
        "    # About the Project\n",
        "    st.subheader(\"About the Project:\")\n",
        "    st.write(\"Our project aims to:\")\n",
        "    st.markdown(\"\"\"\n",
        "    - Update and refine the industrial classification data of main and marginal workers.\n",
        "    - Provide accurate and relevant information for policy-making and employment planning.\n",
        "    - Empower stakeholders with actionable insights to foster economic growth and development.\n",
        "    \"\"\")\n",
        "\n",
        "# -------------------------------------------------------------------------------------------------------------------------------------------------------#\n"
      ],
      "metadata": {
        "id": "mfhKSLNMmXoI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "\n",
        "if selected == \"Overview\":\n",
        "\n",
        "    # Dataset\n",
        "    st.subheader(\"Dataset Overview:\")\n",
        "    st.write(\"Our dataset includes:\")\n",
        "    st.markdown(\"\"\"\n",
        "    - State-wise counts of main and marginal workers across various industries.\n",
        "    - Gender-based distribution of workforce in different sectors.\n",
        "    - Historical data for trend analysis and forecasting.\n",
        "    \"\"\")\n",
        "\n",
        "    # Technologies Used\n",
        "    st.subheader(\"Technologies Utilized:\")\n",
        "    st.write(\"We leverage cutting-edge technologies such as:\")\n",
        "    st.markdown(\"\"\"\n",
        "    - Python for data processing and analysis.\n",
        "    - Streamlit for interactive visualization.\n",
        "    - Plotly and Matplotlib for creating insightful charts.\n",
        "    - NLTK for Natural Language Processing tasks.\n",
        "    \"\"\")\n",
        "\n",
        "    # TF-IDF Vectorization\n",
        "    tfidf_vectorizer = TfidfVectorizer(stop_words='english')\n",
        "    X_tfidf = tfidf_vectorizer.fit_transform(merged_df['NIC Name'])\n",
        "\n",
        "    # KMeans Clustering\n",
        "    num_clusters = 5  # Adjust the number of clusters as needed\n",
        "    kmeans = KMeans(n_clusters=num_clusters, random_state=42)\n",
        "    merged_df['Cluster'] = kmeans.fit_predict(X_tfidf)\n",
        "\n",
        "    # Selectbox for choosing the cluster\n",
        "    selected_cluster = st.selectbox('Select Cluster', range(num_clusters))\n",
        "\n",
        "    # Filter text data for the selected cluster\n",
        "    text_for_cluster = merged_df[merged_df['Cluster'] == selected_cluster]['NIC Name']\n",
        "\n",
        "    # Tokenize and clean text data\n",
        "    tokens = word_tokenize(' '.join(text_for_cluster))\n",
        "    tokens = [word.lower() for word in tokens if word.isalpha()]\n",
        "    stop_words = set(stopwords.words('english'))\n",
        "    tokens = [word for word in tokens if word not in stop_words]\n",
        "\n",
        "    # Count word frequency\n",
        "    word_freq = Counter(tokens)\n",
        "\n",
        "    # Generate word cloud for the selected cluster\n",
        "    wordcloud = WordCloud(width=800, height=400, background_color='white').generate_from_frequencies(dict(word_freq))\n",
        "\n",
        "    # Display the word cloud in Streamlit\n",
        "    st.subheader(f'Word Cloud for Cluster {selected_cluster}')\n",
        "    st.image(wordcloud.to_array(), caption='Word Cloud', use_column_width=True)\n",
        "\n",
        "    # # Streamlit app\n",
        "    # st.title('Cluster Distribution')\n",
        "    #\n",
        "    # # Visualize the clustering results\n",
        "    # st.subheader('Distribution of Clusters (Pie Chart)')\n",
        "    #\n",
        "    # # Count the occurrences of each cluster\n",
        "    # cluster_counts = merged_df['Cluster'].value_counts()\n",
        "    #\n",
        "    # # Convert counts to a pie chart\n",
        "    # fig, ax = plt.subplots()\n",
        "    # ax.pie(cluster_counts, labels=cluster_counts.index, autopct='%1.1f%%', startangle=90)\n",
        "    # ax.axis('equal')  # Equal aspect ratio ensures that pie is drawn as a circle.\n",
        "    # st.pyplot(fig)\n",
        "\n",
        "    # Streamlit app\n",
        "    st.title('Cluster Distribution')\n",
        "\n",
        "    # Visualize the clustering results\n",
        "    st.subheader('Distribution of Clusters (Pie Chart)')\n",
        "\n",
        "    # Count the occurrences of each cluster\n",
        "    cluster_counts = merged_df['Cluster'].value_counts()\n",
        "\n",
        "    # Create a pie chart using Plotly\n",
        "    fig = px.pie(values=cluster_counts.values, names=cluster_counts.index,\n",
        "                 title='Distribution of Clusters',\n",
        "                 labels={'values': 'Count', 'names': 'Clusters'})\n",
        "\n",
        "    # Display the Plotly chart\n",
        "    st.plotly_chart(fig)\n",
        "\n",
        "    # Filter options for work type\n",
        "    work_type_options = ['Main Workers - Total -  Persons', 'Marginal Workers - Total -  Persons']\n",
        "    selected_work_type = st.selectbox(\"Select Work Type:\", work_type_options)\n",
        "\n",
        "    # Filter for top 10 NIC Names based on the selected work type\n",
        "    top_10_nic_names = merged_df.groupby('NIC Name')[selected_work_type].sum().nlargest(10).index\n",
        "    top_10_merged_df = merged_df[merged_df['NIC Name'].isin(top_10_nic_names)]\n",
        "\n",
        "    # Plotting the box plot using Seaborn and Matplotlib\n",
        "    st.subheader(f'Box Plot of {selected_work_type} by Top 10 NIC Name')\n",
        "\n",
        "    # Calculate total values for each NIC Name\n",
        "    top_10_nic_names_totals = top_10_merged_df.groupby('NIC Name')[selected_work_type].sum().reset_index()\n",
        "\n",
        "    # Create the treemap\n",
        "    fig = px.treemap(top_10_nic_names_totals, path=['NIC Name'], values=selected_work_type,\n",
        "                     title=f'Treemap of {selected_work_type} by Top 10 NIC Name')\n",
        "    st.plotly_chart(fig)\n",
        "\n",
        "elif selected == \"Explore\":\n",
        "    # Tokenize and clean text data\n",
        "    text = ' '.join(merged_df['NIC Name'])\n",
        "    tokens = word_tokenize(text)\n",
        "    tokens = [word.lower() for word in tokens if word.isalpha()]\n",
        "    stop_words = set(stopwords.words('english'))\n",
        "    tokens = [word for word in tokens if word not in stop_words]\n",
        "\n",
        "    # Count word frequency\n",
        "    word_freq = Counter(tokens)\n",
        "    top_words = word_freq.most_common(10)\n",
        "\n",
        "    # TF-IDF Vectorization\n",
        "    tfidf_vectorizer = TfidfVectorizer(stop_words='english')\n",
        "    X_tfidf = tfidf_vectorizer.fit_transform(merged_df['NIC Name'])\n",
        "\n",
        "    # KMeans Clustering\n",
        "    num_clusters = 5  # Adjust the number of clusters as needed\n",
        "    kmeans = KMeans(n_clusters=num_clusters, random_state=42)\n",
        "    merged_df['Cluster'] = kmeans.fit_predict(X_tfidf)\n"
      ],
      "metadata": {
        "id": "Qvpv2KhPmXqH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "    # Streamlit App\n",
        "    st.title(\"Industrial Human Resource  Dashboard\")\n",
        "\n",
        "    # Select box for type of worker\n",
        "    worker_type = st.selectbox('Select Worker Type', ['Main Workers', 'Marginal Workers'])\n",
        "\n",
        "    # Column mapping\n",
        "    if worker_type == 'Main Workers':\n",
        "        column_total = 'Main Workers - Total -  Persons'\n",
        "        column_rural = 'Main Workers - Rural -  Persons'\n",
        "        column_urban = 'Main Workers - Urban -  Persons'\n",
        "    else:\n",
        "        column_total = 'Marginal Workers - Total -  Persons'\n",
        "        column_rural = 'Marginal Workers - Rural -  Persons'\n",
        "        column_urban = 'Marginal Workers - Urban -  Persons'\n",
        "\n",
        "    # Strip any extra spaces from column names\n",
        "    merged_df.columns = [col.strip() for col in merged_df.columns]\n",
        "\n",
        "    # Print DataFrame columns for debugging\n",
        "    print(\"DataFrame Columns:\", merged_df.columns)\n",
        "\n",
        "    # Scatter Plot\n",
        "    fig1 = px.scatter(merged_df, x=column_total, y=column_rural, color='Cluster',\n",
        "                      title=f'{worker_type} - Total vs Rural')\n",
        "    st.plotly_chart(fig1)\n",
        "\n",
        "    fig2 = px.scatter(merged_df, x=column_total, y=column_urban, color='Cluster',\n",
        "                      title=f'{worker_type} - Total vs Urban')\n",
        "    st.plotly_chart(fig2)\n",
        "\n",
        "    # # Box Plot for Top 10 NIC Names\n",
        "    # top_10_nic_names = merged_df['NIC Name'].value_counts().head(10).index\n",
        "    # top_10_df = merged_df[merged_df['NIC Name'].isin(top_10_nic_names)]\n",
        "    #\n",
        "    # fig3 = px.box(top_10_df, x='NIC Name', y=column_total, title=f'{worker_type} by Top 10 NIC Names')\n",
        "    # st.plotly_chart(fig3)\n",
        "\n",
        "    # Pie Chart for Top 10 NIC Names\n",
        "    top_10_nic_names = merged_df['NIC Name'].value_counts().head(10)\n",
        "    fig3 = px.pie(top_10_nic_names, values=top_10_nic_names.values, names=top_10_nic_names.index,\n",
        "                  title=f'{worker_type} Distribution by Top 10 NIC Names')\n",
        "\n",
        "    st.plotly_chart(fig3)\n",
        "\n",
        "    # Cluster Distribution\n",
        "    fig4 = px.histogram(merged_df, x='Cluster', title='Cluster Distribution')\n",
        "    st.plotly_chart(fig4)\n",
        "\n",
        "    # # Count plot for a categorical column\n",
        "    # st.subheader(f\"Distribution of {worker_type} by State\")\n",
        "    #\n",
        "    # # Set the color palette\n",
        "    # sns.set_palette(\"bright\")  # You can choose different palettes like \"pastel\", \"deep\", \"bright\", etc.\n",
        "    #\n",
        "    # # Create the plot\n",
        "    # fig, ax = plt.subplots()\n",
        "    # sns.countplot(x='State Name', data=merged_df, ax=ax)\n",
        "    # plt.xticks(rotation=90)\n",
        "    # st.pyplot(fig)\n",
        "\n",
        "    # Bar Chart for Distribution of Worker Type by State\n",
        "    st.subheader(f\"Distribution of {worker_type} by State\")\n",
        "\n",
        "    # Count the occurrences of each state\n",
        "    state_counts = merged_df['State Name'].value_counts()\n",
        "\n",
        "    # Create a bar chart using Plotly\n",
        "    fig = px.bar(state_counts, x=state_counts.index, y=state_counts.values,\n",
        "                 labels={'x': 'State Name', 'y': 'Count'},\n",
        "                 title=f'Distribution of {worker_type} by State')\n",
        "\n",
        "    # Rotate x-axis labels for better visibility\n",
        "    fig.update_layout(xaxis_tickangle=-90)\n",
        "\n",
        "    # Display the Plotly chart\n",
        "    st.plotly_chart(fig)\n",
        "\n",
        "    # Plot\n",
        "    st.subheader(f'Relationship between {worker_type} - Rural/Urban - Persons and {worker_type} - Total - Persons')\n",
        "\n",
        "    # Create the plot\n",
        "    fig, ax = plt.subplots(figsize=(10, 6))\n",
        "    ax.scatter(merged_df[f'{worker_type} - Rural -  Persons'], merged_df[f'{worker_type} - Total -  Persons'],\n",
        "               label='Rural', alpha=0.5)\n",
        "    ax.scatter(merged_df[f'{worker_type} - Urban -  Persons'], merged_df[f'{worker_type} - Total -  Persons'],\n",
        "               label='Urban', alpha=0.5)\n",
        "    ax.set_xlabel(f'{worker_type} - Rural - Persons / {worker_type} - Urban - Persons')\n",
        "    ax.set_ylabel(f'{worker_type} - Total - Persons')\n",
        "    ax.set_title(f'Relationship between {worker_type} - Rural/Urban - Persons and {worker_type} - Total - Persons')\n",
        "    ax.legend()\n",
        "    ax.grid(True)\n",
        "\n",
        "    # Display the plot\n",
        "    st.pyplot(fig)\n",
        "\n",
        "    # Create a word cloud using the top words\n",
        "    wordcloud = WordCloud(width=800, height=400, background_color='white').generate_from_frequencies(dict(top_words))\n",
        "\n",
        "    # Display the word cloud in Streamlit\n",
        "    st.subheader('Word Cloud for Top 10 NIC Names')\n",
        "    st.image(wordcloud.to_array(), caption='Word Cloud', use_column_width=True)\n",
        "\n",
        "    merged_df = merged_df.dropna(subset=['State Name'])\n",
        "\n",
        "\n",
        "    # Fetch GeoJSON data for India's states\n",
        "    @st.cache_resource\n",
        "    def fetch_geojson():\n",
        "        geojson_url = \"https://raw.githubusercontent.com/geohacker/india/master/state/india_state.geojson\"\n",
        "        response = requests.get(geojson_url)\n",
        "        if response.status_code == 200:\n",
        "            return response.json()\n",
        "        else:\n",
        "            st.error(\"Failed to fetch GeoJSON data\")\n",
        "\n",
        "\n",
        "    # Main Streamlit App\n",
        "    def main():\n",
        "        st.title(\"India Map Visualization\")\n",
        "\n",
        "        # Fetch GeoJSON data\n",
        "        geojson_data = fetch_geojson()\n",
        "\n",
        "        # Extract state names from GeoJSON data\n",
        "        geojson_state_names = set(feature['properties']['NAME_1'] for feature in geojson_data['features'])\n",
        "\n",
        "        # State names from DataFrame\n",
        "        dataframe_state_names = set(merged_df['State Name'])\n",
        "\n",
        "        # Select box for type of worker\n",
        "        worker_type = st.selectbox('Select Worker Type', ['Main Workers', 'Marginal Workers'],\n",
        "                                   key=\"worker_type_selectbox\")\n",
        "\n",
        "        # Select box for sex\n",
        "        sex_type = st.selectbox('Select Sex', ['Males', 'Females'], key=\"sex_type_selectbox\")\n",
        "\n",
        "        # Select box for area\n",
        "        area_type = st.selectbox('Select Area', ['Rural', 'Urban'], key=\"area_type_selectbox\")\n",
        "\n",
        "        # Determine the column based on selected worker type, sex, and area\n",
        "        column_name = f'{worker_type} - {area_type} - {sex_type}'\n",
        "\n",
        "        # Plotly Choropleth map\n",
        "        fig = go.Figure(go.Choroplethmapbox(\n",
        "            geojson=geojson_data,\n",
        "            locations=merged_df['State Name'],  # Use the column with state names\n",
        "            featureidkey=\"properties.NAME_1\",  # Key in geojson to match with DataFrame\n",
        "            z=merged_df[column_name],  # Use the column for analysis\n",
        "            colorscale='Viridis',\n",
        "            zmin=merged_df[column_name].min(),\n",
        "            zmax=merged_df[column_name].max(),\n",
        "            marker_opacity=0.7,\n",
        "            marker_line_width=0,\n",
        "        ))\n",
        "\n",
        "        fig.update_layout(\n",
        "            mapbox_style=\"carto-positron\",\n",
        "            mapbox_zoom=3,\n",
        "            mapbox_center={\"lat\": 20.5937, \"lon\": 78.9629},\n",
        "            margin={\"r\": 0, \"t\": 0, \"l\": 0, \"b\": 0},\n",
        "            title=f\"{worker_type} ({sex_type}, {area_type}) Population Across Indian States\",\n",
        "            title_x=0.5\n",
        "        )\n",
        "\n",
        "        # Display the map\n",
        "        st.plotly_chart(fig)\n",
        "\n",
        "        # Top NIC Names State-wise\n",
        "        st.title(\"Top NIC Names State-wise\")\n",
        "        for state in merged_df['State Name'].unique():\n",
        "            top_nic_name = merged_df[merged_df['State Name'] == state]['NIC Name'].mode()[0]\n",
        "            st.write(f\"Top NIC Name in {state}: {top_nic_name}\")\n",
        "\n",
        "\n",
        "    # Call the main function\n",
        "    if __name__ == \"__main__\":\n",
        "        main()"
      ],
      "metadata": {
        "id": "R-q_na9cmXsM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "Hi1AV1PsmXuO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "QBWZWSiBmXyN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "D2Jz3i3XmX0u"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}